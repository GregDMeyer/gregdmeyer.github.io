[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is the personal blog of Greg Kahanamoku-Meyer. You can learn more about me at my personal website, linked below.\nAlthough I always cite the sources I learn things from, there‚Äôs a chance I independently come up with something and post it here unaware of an existing reference for it. If you know of a reference I missed for something that appears on this blog, please let me know!"
  },
  {
    "objectID": "posts/bugout/index.html",
    "href": "posts/bugout/index.html",
    "title": "How I saved Berkeley $20,000",
    "section": "",
    "text": "Here is a fun story from a few years ago.\nMy spouse is a paleontologist (Figure¬†1).\nBut Sara doesn‚Äôt work on, like, T. rex‚Äîthey work on microfossils, in particular these tiny, sand-grained-sized amoebas called foraminifera, which form shells that fossilize.1 (You can read about my spouse‚Äôs work here, if you‚Äôre interested! Yes, they are a lot cooler than me.) Foraminifera (or ‚Äúforams‚Äù for short) are really useful because 1) they‚Äôre everywhere, so you just have to kind of scoop up some ocean dirt and you can find a lot of them and 2) their various characteristics like shape and shell chemistry can help inform how old a rock layer is, and what the conditions were like in the ocean when it formed. Perhaps unsurprisingly, those characteristics are also of great interest to oil companies, in their endless search for oil deposits to exploit (and they were even more so in the era before radiometric dating became commonplace).\nOur story starts in the second half of the 20th century, when the oil company Arco hired some paleontologists to‚Ä¶ dig around, I guess, as paleontologists do. Specifically their goal was to use forams to figure out where Arco should drill for oil.\nAlways interested in increasing the efficiency with which they were destroying our planet, in the ‚Äô80s Arco started digitally recording the data they were collecting, using the early ‚Äúmicrocomputers‚Äù that were becoming widespread at that time. Via some chain of events that I don‚Äôt entirely understand, floppy disks containing that data ended up in the University of California Museum of Paleontology (UCMP), home of Osborne the T. rex, and where my spouse was working on their PhD in 2018. And there were a LOT of floppies (Figure¬†3)‚Äîcontaining a total of 2.5 million records, or so.\nSuch a large dataset seemed ripe with potential for doing cool paleontology studies without even having to go outside,2 so my spouse got their hands on a floppy disk drive, checked out the data, and‚Ä¶ it looked like this:\nIt was all in some weird, mysterious format (rather reminiscent of the terminals in Fallout!). From some discussion with the director of the UCMP who accepted the floppies, Sara found out that the data had been recorded via a proprietary program called BUGIN,3 written in the 80s in FORTRAN for MS-DOS. The director reached out to a contact who was formerly an ARCO employee about converting the files, and the former employee responded that they knew how to do it, but it would cost‚Ä¶ twenty thousand dollars.\n$20,000??? To run an already-existing piece of software against some data? I guess I shouldn‚Äôt be surprised considering the general behavior of oil companies and their employees, but WOW. Even worse, it sounded like the museum was seriously considering going through with it‚Äîmillions of already-collected paleontological records were simply too good to pass up.\nIf anything gets me worked up, it‚Äôs stuff like this. Holding valuable research data hostage in order to price gouge a publicly funded university is like, the definition of depravity. (Also, I‚Äôm not sure if this same ‚Äúformer ARCO employee‚Äù is the one who ‚Äúgifted‚Äù UCMP the floppies in the first place, but if so, that would be truly legendary levels of exploitative behavior).\nI decided to take a look around the web and see if I could find any existing tools for decoding BUGIN data. Unsurprisingly, I just found one hit, a company run by a former ARCO employee, selling a ‚Äúservice‚Äù converting BUGIN files to CSV, as well as other paleontology software (Figure¬†5).\nI poked around their website a bit, and honestly, I can only be impressed at their audacity in selling software. Take for example a program they offer called ‚ÄúMerge Spreadsheets 2,‚Äù which can be yours for the low price of $75. What does ‚ÄúMerge Spreadsheets 2‚Äù do, you ask? It takes two CSV files, and, you guessed it, merges them into one CSV file. This task is also accomplished by the following single line of code on the Linux command line:4\nTruly groundbreaking engineering happening over at PAZ software. (Also, I am now extremely curious what technological innovations went into ‚ÄúMerge Spreadsheets 2‚Äù compared to what was presumably the original ‚ÄúMerge Spreadsheets‚Äù?)\nAnyway, this situation was the perfect nerd snipe: an interesting retro computer problem paired with a little bit of preventing exploitative capitalism? Sorry to the Floquet time crystals I was supposed to be working on, it would have to wait. Time to put on a hoodie, find a dark room somewhere, turn on synthwave, and hack."
  },
  {
    "objectID": "posts/bugout/index.html#footnotes",
    "href": "posts/bugout/index.html#footnotes",
    "title": "How I saved Berkeley $20,000",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nActually a good fraction of the sand in the world is made up of forams!‚Ü©Ô∏é\nAlthough as someone whose job never involves hiking mountains and going on boats, I don‚Äôt understand why anyone whose job could involve that would choose not to!‚Ü©Ô∏é\nApparently the oil people call forams ‚Äúbugs,‚Äù really adding to the Fallout aesthetic.‚Ü©Ô∏é\nOK, if we‚Äôre being totally fair, ‚ÄúMerge Spreadsheets 2‚Äù also deduplicates the columns, or something. But we don‚Äôt need to be fair, because the people who make this software certainly aren‚Äôt!‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/pdf_python/index.html",
    "href": "posts/pdf_python/index.html",
    "title": "Embedding runnable Python in PDFs",
    "section": "",
    "text": "One reason I am excited about starting this blog is because I have a long backlog of ‚Äúdumb stuff I have done with computers‚Äù that I want to share with the world. Here‚Äôs one of them.\nTo show you the idea, check out this PDF."
  },
  {
    "objectID": "posts/pdf_python/index.html#how-it-works",
    "href": "posts/pdf_python/index.html#how-it-works",
    "title": "Embedding runnable Python in PDFs",
    "section": "How it works",
    "text": "How it works\nA little-known (I think) feature of Python is that you can run code directly from a .zip file containing a __main__.py. Check this out:\n\necho 'print(\"it works!\")' &gt; __main__.py\nzip -q code.zip __main__.py\npython code.zip\n\nit works!\n\n\n(See this part of the Python docs.)\nNow consider the following two facts about the structure of ZIP and PDF files:\n\nPDF files start with a header that describes the version and some other information, and end with %%EOF to mark the end of the file.\nZIP files end with a directory describing, via relative byte offsets, where each part of the data in the file is stored.\n\nBut wait. This means that if we just concatenate a PDF and a ZIP file, the result will still start with the PDF header, and be a PDF until %%EOF, and the last part of the file will be the ZIP directory, which describes in relative offsets where to find the ZIP data.\nüëÄ\nSo it‚Äôs a valid file, in both formats!\nIn short, you can make a file that is both a PDF and a ZIP file by simply concatenating the two, and since it is possible to create and run ZIP files as Python, that means you can create a file that both is a valid PDF and runs as Python!\n\nAddendum\nA surprising thing that somewhat undermines the above argument about file structure is that (at least on my machine) it actually still works if you concatenate them in the ‚Äúwrong‚Äù order‚Äîthe ZIP first, then the PDF! I guess the libraries for reading PDFs and ZIP files are really robust to weird/corrupt file structures. I wonder what possibilities this opens up. Could we make a single file that is a PNG, and a PDF, and a ZIP file? It is left for the reader to explore‚Ä¶"
  },
  {
    "objectID": "posts/stable_fidelity/index.html",
    "href": "posts/stable_fidelity/index.html",
    "title": "Numerically computing fidelities close to 1",
    "section": "",
    "text": "TL;DR\n\n\n\nFor two states \\(\\ket{a}\\) and \\(\\ket{b}\\), let \\(c = \\braket{a|b}/|\\braket{a|b}|\\) and \\(e = |c\\ket{a} - \\ket{b}|^2\\) (the norm squared of the difference vector between \\(c\\ket{a}\\) and \\(\\ket{b}\\)). Then the fidelity \\(\\mathcal{F} = |\\braket{a|b}|^2\\) can be computed with good numerical precision as \\[\\mathcal{F} = (1 - e/2)^2\\]\n\n\n\n\n\n\n\n\nNote\n\n\n\nSome authors, including Nielsen and Chuang in their textbook, define the fidelity as \\(\\mathcal{F}' = |\\braket{a|b}|\\), which is the square root of the definition used in this blog post. The technique presented here can easily be adapted to that definition, as \\(\\mathcal{F}' = 1-e/2\\).\n\n\nSuppose we want to examine the fidelity \\(\\mathcal{F} = |\\braket{a|b}|^2\\) of two quantum states \\(\\ket{a}\\) and \\(\\ket{b}\\), stored as vectors of complex numbers. If \\(\\ket{a}\\) and \\(\\ket{b}\\) are very close, computing this value in the obvious way runs into problems with floating-point precision, as we will find with this straightforward function for the fidelity:\n\ndef fidelity_naive(a, b):\n    return np.abs(np.vdot(a, b))**2\n\nLet‚Äôs try plotting the measured fidelity vs.¬†actual fidelity, for a range of fidelities very close to 1. (Here we plot the difference from 1, so that we can see the results easily on a log plot).\n\n\nCode\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef generate_random_complex_unit_vector(dimension):\n    rtn = np.zeros((dimension,), dtype=np.complex64)\n    rtn += np.random.standard_normal(dimension)\n    rtn += 1j * np.random.standard_normal(dimension)\n    rtn /= np.linalg.norm(rtn)\n    return rtn\n\ndef generate_random_orthogonal_vectors(dimension):\n    a = generate_random_complex_unit_vector(dimension)\n    b = generate_random_complex_unit_vector(dimension)\n\n    # gram-schmidt, to make them orthogonal\n    b -= np.vdot(a, b) * a\n    b /= np.linalg.norm(b)\n\n    return a, b\n\ndef plot_fidelity_measure(dimension, x_min, x_max, n_pts, fidelity_funcs):\n    a, b = generate_random_orthogonal_vectors(dimension)\n    \n    pts = np.geomspace(x_min, x_max, n_pts)\n\n    for fidelity_func in fidelity_funcs:\n        results = []\n        for target_err in pts:\n            c = np.sqrt(target_err)*b + np.sqrt(1-target_err)*a\n            results.append(fidelity_func(a, c))\n\n        plt.loglog(pts, 1-np.array(results), label=r\"$1 - $\"+fidelity_func.__name__)\n    \n    plt.loglog(pts, pts, color='k', linestyle=':', label=r'$1 - \\mathcal{F}$')\n\n    plt.xlabel(r'$1-\\mathcal{F}$')\n    plt.ylabel('Result')\n\n    plt.title('Computed vs. actual fidelity')\n\n    plt.legend()\n\nplot_fidelity_measure(\n    dimension=2**20, \n    x_min=1E-14, \n    x_max=1, \n    n_pts=21, \n    fidelity_funcs=[fidelity_naive]\n)\n\n\n\n\n\n\n\n\n\nThe reason for the discrepancy is that in the inner product, we are summing values \\(a_i^* b_i\\) that have magnitude \\(\\mathcal{O}(1/d)\\), but the ‚Äúimportant part‚Äù of each term (that prevents the sum from being exactly 1) is of order \\(\\mathcal{O}(\\epsilon/d)\\), where \\(\\epsilon = 1-\\mathcal{F}\\). If \\(\\epsilon\\) is very small, this ‚Äúcontent‚Äù will be lost to floating point errors. Instead, we‚Äôd like to figure out a way to sum that content we care about directly.\nConsider the following fact: \\[\n|\\ket{a}-\\ket{b}|^2 = 2 - 2\\mathrm{Re}\\left[ \\braket{a | b} \\right]\n\\] So, the square of the norm of the difference gives the ~vibes~ of \\(1 - \\left|\\braket{a|b}\\right|\\), except it cares about the relative phase between \\(\\ket{a}\\) and \\(\\ket{b}\\). The upshot is that it should be numerically stable to compute, because the elements of the difference vector will be of order \\(\\mathcal{O}(\\epsilon/d)\\). The key idea is that we can use it to measure \\(|\\braket{a|b}|\\) if we ensure there is no relative phase between \\(\\ket{a}\\) and \\(\\ket{b}\\).\nLet \\(c = \\braket{a|b}/|\\braket{a|b}|\\). Then by construction \\(\\braket{c a | b} = |\\braket{a | b}| = \\mathrm{Re}\\left[ \\braket{c a | b} \\right]\\). Combining this with the above, we may define \\(e = |c \\ket{a}-\\ket{b}|^2\\) and then: \\[\n|\\braket{a | b}| = 1 - e/2\n\\] and thus \\[\n\\mathcal{F} = |\\braket{a|b}|^2 = (1 - e/2)^2\n\\] or, if we‚Äôre interested in the difference from 1, \\[\n1-\\mathcal{F} = e - e^2/4\n\\] This is an exact expression (we didn‚Äôt use any approximations to get it), but it also will not suffer from numerical instability for values close to zero.\nPretty cool! Let‚Äôs try it:\n\ndef fidelity_precise(a, b):\n    inner_prod = np.vdot(a, b)\n\n    # to avoid division by zero\n    if inner_prod == 0:\n        return 0.0\n        \n    c = inner_prod / np.abs(inner_prod)\n    diff = c*a - b\n    e = np.real(np.vdot(diff, diff))  # imag part is zero\n    return (1-e/2)**2\n\n\n\nCode\nplot_fidelity_measure(\n    dimension=2**20, \n    x_min=1E-14, \n    x_max=1, \n    n_pts=21, \n    fidelity_funcs=[fidelity_naive, fidelity_precise]\n)\n\n\n\n\n\n\n\n\n\nIt works super well! Yay!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ancillary data",
    "section": "",
    "text": "How I saved Berkeley $20,000\n\n\n\n\n\n\nfun\n\n\ncode\n\n\nretro computing\n\n\n\nAnd got repaid in snacks (and deep personal satisfaction).\n\n\n\n\n\nOct 24, 2024\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nEmbedding runnable Python in PDFs\n\n\n\n\n\n\ncode\n\n\nfun\n\n\ndumb stuff with computers\n\n\n\nTired of distributing code with your manuscript? How about distributing your code as your manuscript!\n\n\n\n\n\nOct 12, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nNumerically computing fidelities close to 1\n\n\n\n\n\n\nquantum\n\n\nnumerics\n\n\n\nA trick for avoiding floating-point errors when numerically computing the fidelity of quantum state vectors.\n\n\n\n\n\nOct 10, 2024\n\n\n3 min\n\n\n\n\n\n\nNo matching items"
  }
]